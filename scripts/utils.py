#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»£ç å¥åº·ç›‘æ§ - å·¥å…·å‡½æ•°åº“
Author: DevOps Team
Created: 2025-12-30
"""

import os
import re
import subprocess
from datetime import datetime, timedelta
from typing import List, Dict, Tuple
import yaml


def parse_iso_datetime(date_str: str) -> datetime:
    """è§£æISOæ ¼å¼æ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²ï¼ˆå…¼å®¹Python 3.6ï¼‰"""
    # ç§»é™¤æ—¶åŒºä¿¡æ¯
    date_str = date_str.replace(' +0800', '').replace('+0800', '')
    # Python 3.6å…¼å®¹çš„è§£ææ–¹å¼
    try:
        return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
    except ValueError:
        # å°è¯•å…¶ä»–æ ¼å¼
        return datetime.strptime(date_str, '%Y-%m-%dT%H:%M:%S')


class GitAnalyzer:
    """Git ä»“åº“åˆ†æå™¨"""

    def __init__(self, repo_path: str):
        self.repo_path = repo_path
        self.repo_name = os.path.basename(repo_path)

    def run_git_command(self, command: List[str]) -> str:
        """æ‰§è¡Œ git å‘½ä»¤"""
        try:
            result = subprocess.run(
                ["git", "-C", self.repo_path] + command,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                universal_newlines=True,  # Python 3.6 å…¼å®¹
                check=True
            )
            return result.stdout.strip()
        except subprocess.CalledProcessError as e:
            print(f"Error in {self.repo_name}: {e}")
            return ""

    def get_commits(self, since: str = "1 day ago", until: str = None, branch: str = "all") -> List[Dict]:
        """è·å–æäº¤è®°å½•"""
        cmd = [
            "log",
            f"--since={since}",
            "--pretty=format:%H|%an|%ae|%ad|%s",
            "--date=iso",
            "--numstat"
        ]

        if until:
            cmd.insert(2, f"--until={until}")

        if branch != "all":
            cmd.append(branch)
        else:
            cmd.append("--all")

        output = self.run_git_command(cmd)
        if not output:
            return []

        commits = []
        lines = output.split('\n')
        i = 0

        while i < len(lines):
            if '|' in lines[i]:
                parts = lines[i].split('|')
                commit = {
                    'hash': parts[0],
                    'author': parts[1],
                    'email': parts[2],
                    'date': parts[3],
                    'message': parts[4] if len(parts) > 4 else '',
                    'files': [],
                    'lines_added': 0,
                    'lines_deleted': 0
                }

                i += 1
                # è§£ææ–‡ä»¶å˜æ›´ç»Ÿè®¡
                while i < len(lines) and lines[i] and '|' not in lines[i]:
                    parts = lines[i].split('\t')
                    if len(parts) >= 3:
                        added = parts[0] if parts[0] != '-' else 0
                        deleted = parts[1] if parts[1] != '-' else 0
                        filepath = parts[2]

                        try:
                            added = int(added)
                            deleted = int(deleted)
                        except:
                            added = 0
                            deleted = 0

                        commit['files'].append({
                            'path': filepath,
                            'added': added,
                            'deleted': deleted
                        })
                        commit['lines_added'] += added
                        commit['lines_deleted'] += deleted

                    i += 1

                commits.append(commit)
            else:
                i += 1

        return commits

    def get_file_history(self, filepath: str, since: str = "7 days ago") -> List[Dict]:
        """è·å–æ–‡ä»¶ä¿®æ”¹å†å²"""
        cmd = [
            "log",
            f"--since={since}",
            "--pretty=format:%H|%an|%ad",
            "--date=iso",
            "--",
            filepath
        ]

        output = self.run_git_command(cmd)
        if not output:
            return []

        history = []
        for line in output.split('\n'):
            if '|' in line:
                parts = line.split('|')
                history.append({
                    'hash': parts[0],
                    'author': parts[1],
                    'date': parts[2]
                })

        return history

    def get_all_modified_files(self, since: str = "1 day ago") -> List[str]:
        """è·å–æ‰€æœ‰ä¿®æ”¹è¿‡çš„æ–‡ä»¶"""
        cmd = [
            "log",
            f"--since={since}",
            "--name-only",
            "--pretty=format:",
            "--all"
        ]

        output = self.run_git_command(cmd)
        if not output:
            return []

        files = [f for f in output.split('\n') if f.strip()]
        return list(set(files))

    def get_file_size(self, filepath: str) -> int:
        """è·å–æ–‡ä»¶è¡Œæ•°"""
        full_path = os.path.join(self.repo_path, filepath)
        if not os.path.exists(full_path):
            return 0

        try:
            with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                return sum(1 for _ in f)
        except:
            return 0

    def get_file_authors(self, filepath: str, since: str = "7 days ago") -> set:
        """è·å–æ–‡ä»¶çš„ä½œè€…åˆ—è¡¨"""
        history = self.get_file_history(filepath, since)
        return set(h['author'] for h in history)


class ChurnAnalyzer:
    """ä»£ç éœ‡è¡åˆ†æå™¨"""

    def __init__(self, git_analyzer: GitAnalyzer, churn_days: int = 3, churn_count: int = 5):
        self.git_analyzer = git_analyzer
        self.churn_days = churn_days
        self.churn_count = churn_count

    def analyze(self) -> Tuple[List[Dict], float]:
        """åˆ†æä»£ç éœ‡è¡"""
        # è·å–æœ€è¿‘Nå¤©ä¿®æ”¹çš„æ‰€æœ‰æ–‡ä»¶
        files = self.git_analyzer.get_all_modified_files(f"{self.churn_days} days ago")

        churn_files = []
        for filepath in files:
            history = self.git_analyzer.get_file_history(filepath, f"{self.churn_days} days ago")
            modify_count = len(history)

            if modify_count >= self.churn_count:
                authors = self.git_analyzer.get_file_authors(filepath, f"{self.churn_days} days ago")
                churn_files.append({
                    'file': filepath,
                    'count': modify_count,
                    'authors': list(authors),
                    'size': self.git_analyzer.get_file_size(filepath)
                })

        # è®¡ç®—éœ‡è¡ç‡
        total_files = len(files) if files else 1
        churn_rate = (len(churn_files) / total_files) * 100

        # æŒ‰ä¿®æ”¹æ¬¡æ•°æ’åº
        churn_files.sort(key=lambda x: x['count'], reverse=True)

        return churn_files, churn_rate


class ReworkAnalyzer:
    """è¿”å·¥ç‡åˆ†æå™¨"""

    def __init__(self, git_analyzer: GitAnalyzer, add_days: int = 7, delete_days: int = 3):
        self.git_analyzer = git_analyzer
        self.add_days = add_days
        self.delete_days = delete_days

    def analyze(self) -> Tuple[int, int, float]:
        """åˆ†æè¿”å·¥ç‡
        Returns: (è¿”å·¥è¡Œæ•°, æ€»æ–°å¢è¡Œæ•°, è¿”å·¥ç‡)
        """
        # è·å–æœ€è¿‘Nå¤©çš„æäº¤
        commits = self.git_analyzer.get_commits(f"{self.add_days} days ago")

        # ç»Ÿè®¡æ¯ä¸ªæ–‡ä»¶çš„æ–°å¢å’Œåˆ é™¤
        file_changes = {}
        for commit in commits:
            commit_date = parse_iso_datetime(commit['date'])

            for file_info in commit['files']:
                filepath = file_info['path']
                if filepath not in file_changes:
                    file_changes[filepath] = []

                file_changes[filepath].append({
                    'date': commit_date,
                    'added': file_info['added'],
                    'deleted': file_info['deleted']
                })

        # æ£€æµ‹è¿”å·¥ï¼šNå¤©å†…æ–°å¢ï¼ŒMå¤©å†…è¢«åˆ é™¤
        rework_lines = 0
        total_added = 0

        for filepath, changes in file_changes.items():
            changes.sort(key=lambda x: x['date'])

            for i, change in enumerate(changes):
                total_added += change['added']

                # æ£€æŸ¥è¿™æ¬¡æ–°å¢æ˜¯å¦åœ¨åç»­å‡ å¤©å†…è¢«åˆ é™¤
                for j in range(i + 1, len(changes)):
                    days_diff = (changes[j]['date'] - change['date']).days
                    if days_diff <= self.delete_days:
                        # ç®€åŒ–è®¡ç®—ï¼šå¦‚æœåç»­æœ‰åˆ é™¤ï¼Œè®¤ä¸ºæ˜¯éƒ¨åˆ†è¿”å·¥
                        estimated_rework = min(change['added'], changes[j]['deleted'])
                        rework_lines += estimated_rework

        # è®¡ç®—è¿”å·¥ç‡
        rework_rate = (rework_lines / total_added * 100) if total_added > 0 else 0

        return rework_lines, total_added, rework_rate


class HotspotAnalyzer:
    """é«˜å±æ–‡ä»¶åˆ†æå™¨"""

    def __init__(self, git_analyzer: GitAnalyzer, config: Dict):
        self.git_analyzer = git_analyzer
        self.config = config

    def analyze(self) -> List[Dict]:
        """åˆ†æé«˜å±æ–‡ä»¶"""
        days = self.config.get('hotspot_days', 7)
        files = self.git_analyzer.get_all_modified_files(f"{days} days ago")

        hotspots = []
        for filepath in files:
            # è·³è¿‡æ’é™¤çš„æ–‡ä»¶
            if self._should_exclude(filepath):
                continue

            history = self.git_analyzer.get_file_history(filepath, f"{days} days ago")
            modify_count = len(history)
            file_size = self.git_analyzer.get_file_size(filepath)
            authors = self.git_analyzer.get_file_authors(filepath, f"{days} days ago")

            # è®¡ç®—é£é™©åˆ†æ•°
            risk_score = self._calculate_risk_score(modify_count, file_size, len(authors))

            # è¯†åˆ«é£é™©æ ‡ç­¾
            tags = self._get_risk_tags(modify_count, file_size, len(authors), filepath)

            if risk_score > 40:  # åªè®°å½•ä¸­ç­‰ä»¥ä¸Šé£é™©
                hotspots.append({
                    'file': filepath,
                    'risk_score': risk_score,
                    'modify_count': modify_count,
                    'file_size': file_size,
                    'author_count': len(authors),
                    'authors': list(authors),
                    'tags': tags,
                    'suggestion': self._get_suggestion(tags, file_size)
                })

        # æŒ‰é£é™©åˆ†æ•°æ’åº
        hotspots.sort(key=lambda x: x['risk_score'], reverse=True)

        return hotspots

    def _calculate_risk_score(self, modify_count: int, file_size: int, author_count: int) -> float:
        """è®¡ç®—é£é™©åˆ†æ•°"""
        freq_score = min(modify_count / 10 * 100, 100)
        size_score = min(file_size / 1000 * 100, 100)
        author_score = min(author_count / 5 * 100, 100)

        risk = (freq_score * 0.3 + size_score * 0.25 + author_score * 0.2)
        return round(risk, 2)

    def _get_risk_tags(self, modify_count: int, file_size: int, author_count: int, filepath: str) -> List[str]:
        """è·å–é£é™©æ ‡ç­¾"""
        tags = []

        if modify_count >= self.config.get('hotspot_count', 10):
            tags.append("é«˜é¢‘ä¿®æ”¹")

        if file_size >= self.config.get('large_file', 1000):
            tags.append("å¤§å‹æ–‡ä»¶")

        if author_count >= self.config.get('multi_author_count', 3):
            tags.append("å¤šäººåä½œ")

        # åŸºäºæ–‡ä»¶ç±»å‹åˆ¤æ–­å¤æ‚åº¦
        if filepath.endswith('.java'):
            # ç®€åŒ–ï¼šå‡è®¾å¤§æ–‡ä»¶ = å¤æ‚æ–‡ä»¶
            if file_size > 800:
                tags.append("å¤æ‚æ–‡ä»¶")
        elif filepath.endswith('.py'):
            if file_size > 500:
                tags.append("å¤æ‚æ–‡ä»¶")

        return tags

    def _get_suggestion(self, tags: List[str], file_size: int) -> str:
        """è·å–æ”¹è¿›å»ºè®®"""
        if "å¤§å‹æ–‡ä»¶" in tags and "å¤æ‚æ–‡ä»¶" in tags:
            return "ğŸ”´ å»ºè®®æ‹†åˆ†æ–‡ä»¶ï¼Œæå–å…¬å…±é€»è¾‘"
        elif "é«˜é¢‘ä¿®æ”¹" in tags:
            return "ğŸŸ  å»ºè®®ç¨³å®šæ¥å£ï¼Œå‡å°‘é¢‘ç¹ä¿®æ”¹"
        elif "å¤šäººåä½œ" in tags:
            return "ğŸŸ¡ å»ºè®®æ˜ç¡®æ¨¡å—èŒè´£ï¼Œå‡å°‘åä½œå†²çª"
        else:
            return "ğŸŸ¢ ä¿æŒå…³æ³¨"

    def _should_exclude(self, filepath: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ’é™¤æ­¤æ–‡ä»¶"""
        exclude_patterns = self.config.get('exclude_patterns', [])
        exclude_dirs = self.config.get('exclude_dirs', [])

        # æ£€æŸ¥ç›®å½•
        for dir_pattern in exclude_dirs:
            if dir_pattern in filepath:
                return True

        # æ£€æŸ¥æ–‡ä»¶æ¨¡å¼
        for pattern in exclude_patterns:
            if pattern.startswith('*.'):
                ext = pattern[1:]
                if filepath.endswith(ext):
                    return True
            elif pattern in filepath:
                return True

        return False


class HealthScoreCalculator:
    """å¥åº·è¯„åˆ†è®¡ç®—å™¨"""

    def __init__(self, config: Dict):
        self.config = config

    def calculate(self, metrics: Dict) -> Tuple[float, List[str]]:
        """è®¡ç®—å¥åº·è¯„åˆ†
        Returns: (åˆ†æ•°, æ‰£åˆ†åŸå› åˆ—è¡¨)
        """
        score = 100.0
        deductions = []

        # å¤§æäº¤æ‰£åˆ†
        large_commits = metrics.get('large_commits', 0)
        if large_commits > 0:
            deduction = large_commits * 5
            score -= deduction
            deductions.append(f"å¤§æäº¤ ({large_commits}æ¬¡): -{deduction}åˆ†")

        # éœ‡è¡ç‡æ‰£åˆ†
        churn_rate = metrics.get('churn_rate', 0)
        if churn_rate > 30:
            deduction = 20
            score -= deduction
            deductions.append(f"é«˜éœ‡è¡ç‡ ({churn_rate:.1f}%): -{deduction}åˆ†")
        elif churn_rate > 10:
            deduction = 10
            score -= deduction
            deductions.append(f"ä¸­ç­‰éœ‡è¡ç‡ ({churn_rate:.1f}%): -{deduction}åˆ†")

        # è¿”å·¥ç‡æ‰£åˆ†
        rework_rate = metrics.get('rework_rate', 0)
        if rework_rate > 30:
            deduction = 15
            score -= deduction
            deductions.append(f"é«˜è¿”å·¥ç‡ ({rework_rate:.1f}%): -{deduction}åˆ†")
        elif rework_rate > 15:
            deduction = 8
            score -= deduction
            deductions.append(f"ä¸­ç­‰è¿”å·¥ç‡ ({rework_rate:.1f}%): -{deduction}åˆ†")

        # æäº¤ä¿¡æ¯è´¨é‡æ‰£åˆ†
        message_quality = metrics.get('message_quality', 100)
        if message_quality < 60:
            deduction = 10
            score -= deduction
            deductions.append(f"æäº¤ä¿¡æ¯è´¨é‡å·® ({message_quality:.0f}%): -{deduction}åˆ†")

        # æ·±å¤œ/å‘¨æœ«å·¥ä½œæ‰£åˆ†
        late_commits = metrics.get('late_night_commits', 0)
        weekend_commits = metrics.get('weekend_commits', 0)
        abnormal_commits = late_commits + weekend_commits
        if abnormal_commits > 0:
            deduction = abnormal_commits * 2
            score -= deduction
            deductions.append(f"å¼‚å¸¸å·¥ä½œæ—¶é—´ ({abnormal_commits}æ¬¡): -{deduction}åˆ†")

        # æœªå¤„ç†é«˜å±æ–‡ä»¶æ‰£åˆ†
        high_risk_files = metrics.get('high_risk_files', 0)
        if high_risk_files > 0:
            deduction = min(high_risk_files * 3, 15)
            score -= deduction
            deductions.append(f"é«˜å±æ–‡ä»¶ ({high_risk_files}ä¸ª): -{deduction}åˆ†")

        score = max(0, score)
        return round(score, 1), deductions

    def get_level(self, score: float) -> Tuple[str, str]:
        """è·å–è¯„åˆ†ç­‰çº§
        Returns: (ç­‰çº§emoji, ç­‰çº§æè¿°)
        """
        if score >= self.config.get('health_score_excellent', 80):
            return "ğŸŸ¢", "ä¼˜ç§€"
        elif score >= self.config.get('health_score_good', 60):
            return "ğŸŸ¡", "è‰¯å¥½"
        elif score >= self.config.get('health_score_warning', 40):
            return "ğŸŸ ", "è­¦å‘Š"
        else:
            return "ğŸ”´", "å±é™©"


def load_config(config_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def format_number(num: int) -> str:
    """æ ¼å¼åŒ–æ•°å­—ï¼Œæ·»åŠ åƒåˆ†ä½"""
    return f"{num:,}"


def get_time_range(hours: int = 24) -> str:
    """è·å–æ—¶é—´èŒƒå›´å­—ç¬¦ä¸²"""
    if hours == 24:
        return "1 day ago"
    elif hours == 168:  # 7 days
        return "7 days ago"
    else:
        return f"{hours} hours ago"


def is_late_night(time_str: str, config: Dict) -> bool:
    """åˆ¤æ–­æ˜¯å¦æ·±å¤œæäº¤"""
    try:
        time = parse_iso_datetime(time_str)
        hour = time.hour

        late_start = int(config['working_hours']['late_night_start'].split(':')[0])
        late_end = int(config['working_hours']['late_night_end'].split(':')[0])

        if late_start > late_end:  # è·¨å¤©
            return hour >= late_start or hour < late_end
        else:
            return late_start <= hour < late_end
    except:
        return False


def is_weekend(time_str: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦å‘¨æœ«æäº¤"""
    try:
        time = parse_iso_datetime(time_str)
        return time.weekday() >= 5  # 5=Saturday, 6=Sunday
    except:
        return False


def is_overtime(time_str: str, config: Dict) -> bool:
    """åˆ¤æ–­æ˜¯å¦åŠ ç­æ—¶é—´æäº¤
    åŠ ç­æ—¶é—´å®šä¹‰ä¸º: 18:00-21:00
    """
    try:
        time = parse_iso_datetime(time_str)
        hour = time.hour
        minute = time.minute

        # ä»é…ç½®è¯»å–åŠ ç­æ—¶é—´æ®µ
        overtime_start = config.get('working_hours', {}).get('overtime_start', '18:00')
        overtime_end = config.get('working_hours', {}).get('overtime_end', '21:00')

        start_hour, start_minute = map(int, overtime_start.split(':'))
        end_hour, end_minute = map(int, overtime_end.split(':'))

        # è½¬æ¢ä¸ºåˆ†é’Ÿè¿›è¡Œæ¯”è¾ƒ
        current_minutes = hour * 60 + minute
        start_minutes = start_hour * 60 + start_minute
        end_minutes = end_hour * 60 + end_minute

        return start_minutes <= current_minutes < end_minutes
    except:
        return False


def calculate_message_quality(commits: List[Dict]) -> float:
    """è®¡ç®—æäº¤ä¿¡æ¯è´¨é‡"""
    if not commits:
        return 100.0

    good_patterns = [
        r'^(feat|fix|refactor|docs|test|chore|style|perf)(\(.+\))?:',
        r'.{10,}',  # è‡³å°‘10ä¸ªå­—ç¬¦
    ]

    good_count = 0
    for commit in commits:
        message = commit.get('message', '')
        is_good = any(re.match(pattern, message) for pattern in good_patterns)
        if is_good:
            good_count += 1

    return (good_count / len(commits)) * 100
